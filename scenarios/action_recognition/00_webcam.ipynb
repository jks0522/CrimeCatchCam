{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2bd54cdb-aa39-48f9-8bb0-d602135c1b6c"
    }
   },
   "source": [
    "# Quickstart: Web Cam Action Recognition\n",
    "\n",
    "Action recognition is the increasingly popular computer vision task of determining specific actions in a given video.\n",
    "\n",
    "This notebook shows a simple example of loading a pretrained R(2+1)D model for action recognition and using a webcam stream to identify what actions are being performed.\n",
    "\n",
    "For more details about the underlying technology of action recognition, including finetuning, please see our [training introduction notebook](01_training_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite for Webcam example \n",
    "This notebook assumes you have **a webcam** connected to your machine.  We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on **Chrome** and **Firefox**. For more details about the widget, please visit `ipywebrtc` [github](https://github.com/maartenbreddels/ipywebrtc) or [documentation](https://ipywebrtc.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "89d4b446-c421-488c-8967-8a38e538a9b2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)] \n",
      "\n",
      "PyTorch 1.2.0 \n",
      "\n",
      "Torch-vision 0.4.0 \n",
      "\n",
      "Available devices:\n",
      "0: GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "# Regular Python libraries\n",
    "import datetime\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "from collections import deque #\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "from time import sleep, time\n",
    "from threading import Thread\n",
    "from IPython.display import Video\n",
    "\n",
    "# Third party tools\n",
    "import decord #\n",
    "import IPython.display #\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywidgets import HBox, HTML, Layout, VBox, Widget, Label\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# utils_cv\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.action_recognition.data import KINETICS, Urls\n",
    "from utils_cv.action_recognition.dataset import get_transforms\n",
    "from utils_cv.action_recognition.model import VideoLearner\n",
    "from utils_cv.action_recognition.references import transforms_video as transforms\n",
    "from utils_cv.common.gpu import system_info, torch_device\n",
    "from utils_cv.common.data import data_path\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "6db47566-8c49-44bb-aa28-c63bf3fb63cd"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf40a98f-414d-4827-8aa8-88ef6b9cc626"
    }
   },
   "source": [
    "## Load Pre-trained Model\n",
    "\n",
    "Load R(2+1)D 34-layer model pre-trained on IG65M or Kinetics400. There are also two versions of the model that we provide by default: an 8-frame model and 32-frame model based on the input clip length. As you'd expect, the 32-frame model is slower than 8-frame model. \n",
    "\n",
    "We'll start by setting some of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "c456a2db-b030-46fc-8cd7-55c133feb354"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_FRAMES = 8  # 8 or 32.\n",
    "IM_SCALE = 128  # resize then crop\n",
    "INPUT_SIZE = 112  # input clip size: 3 x NUM_FRAMES x 112 x 112\n",
    "\n",
    "# video sample to download\n",
    "sample_video_url = Urls.webcam_vid\n",
    "\n",
    "# file path to save video sample\n",
    "video_fpath = data_path() / \"sample_video.mp4\"\n",
    "\n",
    "# prediction score threshold\n",
    "SCORE_THRESHOLD = 0.01\n",
    "\n",
    "# Averaging 5 latest clips to make video-level prediction (or smoothing)\n",
    "AVERAGING_SIZE = 5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just need to initialize our VideoLearner model and add the parameters we set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "c6f4b85d-2f55-4d5a-8edb-5a9b9f8cc8b4"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading r2plus1d_34_8_ig65m model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dnsgm/.cache\\torch\\hub\\moabitcoin_ig65m-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "learner = VideoLearner(\n",
    "    sample_length=NUM_FRAMES,\n",
    "    num_classes=11\n",
    ")\n",
    "learner.load('crimecatchcam', 'C:/Users/dnsgm/.cache/torch/checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8cec9405-d125-4681-9bdd-ec2184257de3"
    }
   },
   "source": [
    "## Prepare class names and prediction variables\n",
    "Since we use Kinetics400 model out of the box, we load its class names. The dataset consists of 400 human actions. For example, the first 10 labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "eb1dfefe-dfe4-46e4-8482-9b2bc1d05b89"
    }
   },
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"faint\",\n",
    "    \"lying\",\n",
    "    \"sitdown\",\n",
    "    \"standup\",\n",
    "    \"staggering\",\n",
    "    \"fallingdown\",\n",
    "    \"headache\",\n",
    "    \"punch\",\n",
    "    \"kicking\",\n",
    "    \"hugging\",\n",
    "    \"wieldknife\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e7de627-d6db-422c-a406-91d15c93985c"
    }
   },
   "source": [
    "Among them, we will use 50 classes that we are interested in (i.e. the actions make sense to demonstrate in front of the webcam) and ignore other classes by filtering out from the model outputs. This will help us reduce the noise during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "cb834095-db67-4b4b-87c3-a133a848a484"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_LABELS = [\n",
    "    \"faint\",\n",
    "]\n",
    "len(TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b3f708a-26d1-4a0c-bf1c-86fcf842751b"
    }
   },
   "source": [
    "# Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From your webcam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll run the same model for prediction actions using our webcam. First we'll set up the webcam params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open(\"time.txt\", 'w')\n",
    "txt_file = open(\"action.txt\", 'w')\n",
    "if os.path.isfile(\"capture.jpeg\"):\n",
    "    os.remove(\"capture.jpeg\")\n",
    "\n",
    "# Webcam settings\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        \"facing_mode\": \"user\",\n",
    "        \"audio\": False,\n",
    "        \"video\": {\"width\": 400, \"height\": 400},\n",
    "    },\n",
    "    layout=Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = ImageRecorder(\n",
    "    format=\"jpg\", stream=w_cam, layout=Layout(padding=\"0 0 0 100px\")\n",
    ")\n",
    "\n",
    "# Text widget to show our classification results\n",
    "w_text = HTML(layout=Layout(padding=\"0 0 0 100px\"))\n",
    "\n",
    "text_count = 0\n",
    "frame_num = 0\n",
    "\n",
    "def predict_webcam_frames():\n",
    "    \"\"\" Predict activity by using a pretrained model\n",
    "    \"\"\"\n",
    "    global w_imrecorder, w_text, is_playing\n",
    "    global device, model\n",
    "\n",
    "    # Use deque for sliding window over frames\n",
    "    window = deque()\n",
    "    scores_cache = deque()\n",
    "    scores_sum = np.zeros(len(LABELS))\n",
    "    \n",
    "    open(\"timeaction.txt\", 'w')\n",
    "    if os.path.isdir('images'):\n",
    "            shutil.rmtree(os.path.abspath('.') +  '/images')\n",
    "    os.mkdir('images')\n",
    "    \n",
    "    while is_playing:\n",
    "        try:\n",
    "            global frame_num\n",
    "            # Get the image (RGBA) and convert to RGB\n",
    "            im = Image.open(io.BytesIO(w_imrecorder.image.value)).convert(\"RGB\")\n",
    "                \n",
    "            window.append(np.array(im))\n",
    "            \n",
    "            \n",
    "            # update println func\n",
    "            def update_println(println):\n",
    "                w_text.value = println\n",
    "            \n",
    "            def print_action(actionprint):\n",
    "                global text_count\n",
    "                if frame_num % 10 == 0 and actionprint != '':\n",
    "                    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                    txt_file = open(\"time.txt\", 'w')\n",
    "                    txt_file.write(str(now) + '\\n')\n",
    "                    txt_file = open(\"action.txt\", 'w')\n",
    "                    txt_file.write(str(actionprint) + '\\n')\n",
    "                    txt_file = open(\"timeaction.txt\", 'a')\n",
    "                    text_count = text_count + 1\n",
    "                    txt_file.write(str(text_count) + ' ' + str(now) + ', ' + str(actionprint) + '\\n')\n",
    "                    im.save(\"capture.jpeg\", \"jpeg\")\n",
    "                    im.save(\"images/capture\" + str(round(text_count)) + \".jpeg\", \"jpeg\")\n",
    "                \n",
    "            frame_num = frame_num + 1\n",
    "            \n",
    "            if len(window) == NUM_FRAMES:\n",
    "                learner.predict_frames(\n",
    "                    window,\n",
    "                    scores_cache,\n",
    "                    scores_sum,\n",
    "                    None,\n",
    "                    AVERAGING_SIZE,\n",
    "                    SCORE_THRESHOLD,\n",
    "                    LABELS,\n",
    "                    TARGET_LABELS,\n",
    "                    get_transforms(train=False), \n",
    "                    update_println,\n",
    "                    print_action,\n",
    "                )\n",
    "            else:\n",
    "                w_text.value = \"Preparing...\"\n",
    "        except OSError:\n",
    "            # If im_recorder doesn't have valid image data, skip it.\n",
    "            pass\n",
    "        except BaseException as e:\n",
    "            w_text.value = \"Exception: \" + str(e)\n",
    "            break\n",
    "\n",
    "        # Taking the next snapshot programmatically\n",
    "        w_imrecorder.recording = True\n",
    "        sleep(0.02)\n",
    "\n",
    "is_playing = False\n",
    "#  Once prediciton started, hide image recorder widget for faster fps\n",
    "def start(_):\n",
    "    global is_playing\n",
    "    # Make sure this get called only once\n",
    "    if not is_playing:\n",
    "        w_imrecorder.layout.display = \"none\"\n",
    "        is_playing = True\n",
    "        Thread(target=predict_webcam_frames).start()\n",
    "\n",
    "\n",
    "w_imrecorder.image.observe(start, \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b2047f3-87a7-47bd-a53c-2fa90540be99"
    }
   },
   "source": [
    "To start inference on webcam stream, click 'capture' button when the stream is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 400, 'heighâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HBox([w_cam, w_imrecorder, w_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "acf10288-9c35-45ec-8b20-cbb3782dead3"
    }
   },
   "source": [
    "## Stop Webcam and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "a218abeb-4dd9-4030-9e54-550415ae8a89"
    }
   },
   "outputs": [],
   "source": [
    "is_playing = False\n",
    "Widget.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the webcam by running the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about fine-tuning action recognition models in our next [01_training_introduction.ipynb](01_training_introduction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
